{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upgrading our MNIST Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLDatasets\n",
    "train_x, train_y = MNIST.traindata()\n",
    "test_x,  test_y  = MNIST.testdata();\n",
    "\n",
    "(images, labels) = (reshape(train_x[:,:,1:1000], (1000, 28*28)), train_y[1:1000])\n",
    "one_hot_labels = zeros(length(labels), 10)\n",
    "for (i,l) in enumerate(labels)\n",
    "    one_hot_labels[i,l+1] = 1.0\n",
    "end\n",
    "labels = one_hot_labels\n",
    "\n",
    "test_images = reshape(test_x, (size(test_x)[end], 28*28))\n",
    "test_labels = zeros((size(test_x)[end],10))\n",
    "\n",
    "for (i,l) in enumerate(test_y)\n",
    "    test_labels[i,l+1] = 1.0\n",
    "end\n",
    "\n",
    "using Random\n",
    "Random.seed!(1)\n",
    "\n",
    "tanh2deriv(output) = 1 - output^2\n",
    "\n",
    "function softmax(x)\n",
    "    temp = exp.(x)\n",
    "    return temp ./ sum(temp)\n",
    "end\n",
    "\n",
    "function softmax(x):\n",
    "    temp = np.exp(x)\n",
    "    return temp ./ sum(temp, dims=2)\n",
    "end\n",
    "\n",
    "alpha, iterations, hidden_size = (2, 300, 100)\n",
    "pixels_per_image, num_labels = (784, 10)\n",
    "batch_size = 100\n",
    "\n",
    "weights_0_1 = 0.2 .* rand(pixels_per_image,hidden_size) .- 0.1\n",
    "weights_1_2 = 0.2 .* rand(hidden_size,num_labels) .- 0.1\n",
    "\n",
    "for j = 1:iterations\n",
    "    Error, Correct_cnt = (0.0, 0)\n",
    "    for i = 1:batch_size:size(images)[1]-batch_size\n",
    "        batch_start, batch_end = i, i+batch_size-1\n",
    "        layer_0 = images[batch_start:batch_end,:]\n",
    "        layer_1 = tanh.(layer_0 * weights_0_1)\n",
    "        dropout_mask = bitrand(size(layer_1))\n",
    "        layer_1 .*= (dropout_mask .* 2)\n",
    "        layer_2 = softmax(layer_1 * weights_1_2)\n",
    "        \n",
    "        Error += sum((labels[batch_start:batch_end,:] .- layer_2) .^ 2)\n",
    "        \n",
    "        for k=1:batch_size\n",
    "            Correct_cnt += Int(argmax(layer_2[k,:]) == argmax(labels[batch_start+k-1,:]))\n",
    "            layer_2_delta = (labels[batch_start:batch_end,:] .- layer_2) ./batch_size\n",
    "            layer_1_delta = (layer_2_delta * weights_1_2') .* relu2deriv.(layer_1)\n",
    "\n",
    "            layer_1_delta .*= dropout_mask\n",
    "\n",
    "            weights_1_2 += alpha .* layer_1 * layer_2_delta\n",
    "            \n",
    "            println(size(layer_1), size(layer_2_delta))\n",
    "            @assert false\n",
    "            weights_0_1 += alpha .* layer_0' * layer_1_delta\n",
    "        end\n",
    "    end\n",
    "        \n",
    "    if (j % 10 == 0)\n",
    "        test_Error, test_Correct_cnt = (0.0, 0)\n",
    "        for i = 1:size(test_images)[1]\n",
    "\n",
    "            layer_0 = test_images[i,:]\n",
    "            layer_1 = relu.(layer_0' * weights_0_1)\n",
    "            layer_2 = layer_1 * weights_1_2\n",
    "\n",
    "            test_Error += sum((test_labels[i,:]' .- layer_2) .^ 2)\n",
    "            test_Correct_cnt += Int(argmax(layer_2) == argmax(test_labels[i,:]'))\n",
    "        end\n",
    "        println(\"I: $(j) Train error: $(Error/size(images)[1])% Train accuracy: $(Correct_cnt/size(images)[1]) Test-Err:: $(test_Error/size(test_images)[1])% Test-Acc:: $(test_Correct_cnt/size(test_images)[1])\")\n",
    "    end\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.0",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
