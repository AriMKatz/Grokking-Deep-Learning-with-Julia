{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Layer Network on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLDatasets\n",
    "train_x, train_y = MNIST.traindata()\n",
    "test_x,  test_y  = MNIST.testdata();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error: 0.9991914936776255% Train accuracy: 0.112   \r"
     ]
    }
   ],
   "source": [
    "(images, labels) = (reshape(train_x[:,:,1:1000], (1000, 28*28)) ./ 255.0, train_y[1:1000])\n",
    "one_hot_labels = zeros(length(labels), 10)\n",
    "for (i,l) in enumerate(labels)\n",
    "    one_hot_labels[i,l+1] = 1.0\n",
    "end\n",
    "labels = one_hot_labels\n",
    "\n",
    "test_images = reshape(test_x, (size(test_x)[end], 28*28)) ./ 255.0\n",
    "test_labels = zeros((length(test_y),10))\n",
    "\n",
    "for (i,l) in enumerate(test_y)\n",
    "    test_labels[i,l+1] = 1.0\n",
    "end\n",
    "\n",
    "using Random\n",
    "Random.seed!(1)\n",
    "\n",
    "\n",
    "relu(x) = x > 0 ? x : 0\n",
    "relu2deriv(x) = x > 0 ? 1 : 0\n",
    "\n",
    "alpha, iterations, hidden_size, pixels_per_image, num_labels = (0.005, 350, 40, 784, 10)\n",
    "\n",
    "weights_0_1 = 0.2 .* rand(pixels_per_image,hidden_size) .- 0.1\n",
    "weights_1_2 = 0.2 .* rand(hidden_size,num_labels) .- 0.1\n",
    "\n",
    "for j = 1:iterations\n",
    "    Error, Correct_cnt = (0.0, 0)\n",
    "    for i = 1:size(images)[1]\n",
    "        layer_0 = images[i,:]\n",
    "        layer_1 = relu.(layer_0' * weights_0_1)\n",
    "        layer_2 = layer_1 * weights_1_2\n",
    "\n",
    "        Error += sum((labels[i,:]' .- layer_2) .^ 2)\n",
    "        Correct_cnt += Int(argmax(layer_2) == argmax(labels[i,:]'))\n",
    "\n",
    "        layer_2_delta = (labels[i]' .- layer_2)\n",
    "        layer_1_delta = (layer_2_delta * weights_1_2') .* relu2deriv.(layer_1)\n",
    "        weights_1_2 += alpha .* layer_1' * layer_2_delta\n",
    "        weights_0_1 += alpha .* layer_0 * layer_1_delta\n",
    "    end\n",
    "    print(\"Train error: $(Error/size(images)[1])% Train accuracy: $(Correct_cnt/size(images)[1])   \\r\")\n",
    "end\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(j % 10 == 0 or j == iterations)\n",
    "    Error, Correct_cnt = (0.0, 0)\n",
    "\n",
    "    for i = 1:size(test_images)[1]\n",
    "        \n",
    "        layer_0 = test_images[i,:]\n",
    "        layer_1 = relu.(layer_0' * weights_0_1)\n",
    "        layer_2 = layer_1 * weights_1_2\n",
    "        \n",
    "        Error += sum((test_labels[i,:]' .- layer_2) .^ 2)\n",
    "        Correct_cnt += Int(argmax(layer_2) == argmax(test_labels[i,:]'))\n",
    "\n",
    "\n",
    "    end\n",
    "    print(\"Test-Err:: $(Error/size(test_images)[1])% Test-Acc:: $(Correct_cnt/size(test_images)[1])   \\r\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "DimensionMismatch",
     "evalue": "DimensionMismatch(\"A has dimensions (1,10) but B has dimensions (40,10)\")",
     "output_type": "error",
     "traceback": [
      "DimensionMismatch(\"A has dimensions (1,10) but B has dimensions (40,10)\")",
      "",
      "Stacktrace:",
      " [1] gemm_wrapper!(::Array{Float64,2}, ::Char, ::Char, ::Array{Float64,2}, ::Array{Float64,2}, ::LinearAlgebra.MulAddMul{true,true,Bool,Bool}) at /Users/julia/buildbot/worker/package_macos64/build/usr/share/julia/stdlib/v1.4/LinearAlgebra/src/matmul.jl:569",
      " [2] mul! at /Users/julia/buildbot/worker/package_macos64/build/usr/share/julia/stdlib/v1.4/LinearAlgebra/src/matmul.jl:169 [inlined]",
      " [3] mul! at /Users/julia/buildbot/worker/package_macos64/build/usr/share/julia/stdlib/v1.4/LinearAlgebra/src/matmul.jl:208 [inlined]",
      " [4] *(::Array{Float64,2}, ::Array{Float64,2}) at /Users/julia/buildbot/worker/package_macos64/build/usr/share/julia/stdlib/v1.4/LinearAlgebra/src/matmul.jl:160",
      " [5] top-level scope at ./In[133]:38"
     ]
    }
   ],
   "source": [
    "(images, labels) = (reshape(train_x[:,:,1:1000], (1000, 28*28)) ./ 255.0, train_y[1:1000])\n",
    "one_hot_labels = zeros(length(labels), 10)\n",
    "for (i,l) in enumerate(labels)\n",
    "    one_hot_labels[i,l+1] = 1.0\n",
    "end\n",
    "labels = one_hot_labels\n",
    "\n",
    "test_images = reshape(test_x, (size(test_x)[end], 28*28)) ./ 255.0\n",
    "test_labels = zeros((length(test_y),10))\n",
    "\n",
    "for (i,l) in enumerate(test_y)\n",
    "    test_labels[i,l+1] = 1.0\n",
    "end\n",
    "\n",
    "using Random\n",
    "Random.seed!(1)\n",
    "\n",
    "\n",
    "relu(x) = x > 0 ? x : 0\n",
    "relu2deriv(x) = x > 0 ? 1 : 0\n",
    "\n",
    "alpha, iterations, hidden_size, pixels_per_image, num_labels = (0.005, 350, 40, 784, 10)\n",
    "\n",
    "weights_0_1 = 0.2 .* rand(pixels_per_image,hidden_size) .- 0.1\n",
    "weights_1_2 = 0.2 .* rand(hidden_size,num_labels) .- 0.1\n",
    "\n",
    "for j = 1:iterations\n",
    "    Error, Correct_cnt = (0.0, 0)\n",
    "    for i = 1:size(images)[1]\n",
    "        layer_0 = images[i,:]\n",
    "        layer_1 = relu.(layer_0' * weights_0_1)\n",
    "        layer_2 = layer_1 * weights_1_2\n",
    "\n",
    "        Error += sum((labels[i,:]' .- layer_2) .^ 2)\n",
    "        Correct_cnt += Int(argmax(layer_2) == argmax(labels[i,:]'))\n",
    "\n",
    "        layer_2_delta = (labels[i] .- layer_2)\n",
    "        layer_1_delta = (layer_2_delta * weights_1_2) .* relu2deriv.(layer_1)\n",
    "        weights_1_2 += alpha .* layer_1 * layer_2_delta\n",
    "        weights_0_1 += alpha .* layer_0 * layer_1_delta\n",
    "    end\n",
    "    print(\"I: $(j) Train error: $(Error/size(images)[1])% Train accuracy: $(Correct_cnt/size(images)[1])   \\r\")\n",
    "    \n",
    "    if ((j % 10 == 0) || (j == iterations))\n",
    "        println()\n",
    "        Error, Correct_cnt = (0.0, 0)\n",
    "        for i = 1:size(test_images)[1]\n",
    "        \n",
    "            layer_0 = test_images[i,:]\n",
    "            layer_1 = relu.(layer_0' * weights_0_1)\n",
    "            layer_2 = layer_1 * weights_1_2\n",
    "\n",
    "            Error += sum((test_labels[i,:]' .- layer_2) .^ 2)\n",
    "            Correct_cnt += Int(argmax(layer_2) == argmax(test_labels[i,:]'))\n",
    "        end\n",
    "        print(\"Test-Err:: $(Error/size(test_images)[1])% Test-Acc:: $(Correct_cnt/size(test_images)[1]) \\r\")\n",
    "        println()\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout In Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "layer_0 = images[i,:]\n",
    "dropout_mask = bitrand(size(layer_1))\n",
    "layer_1 = relu.(layer_0' * weights_0_1)\n",
    "layer_1 .*= dropout_mask .* 2\n",
    "layer_2 = layer_1 * weights_1_2\n",
    "\n",
    "Error += sum((labels[i,:]' .- layer_2) .^ 2)\n",
    "Correct_cnt += Int(argmax(layer_2) == argmax(labels[i,:]'))\n",
    "\n",
    "layer_2_delta = (labels[i]' .- layer_2)\n",
    "layer_1_delta = (layer_2_delta * weights_1_2') .* relu2deriv.(layer_1)\n",
    "\n",
    "layer_1_delta .*= dropout_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 10 Train error: 0.998787979997298% Train accuracy: 0.095 Test-Err:: 0.09987879799972979% Test-Acc:: 0.0095\n",
      "I: 20 Train error: 0.9978306286667931% Train accuracy: 0.097 Test-Err:: 0.0997830628666793% Test-Acc:: 0.0097\n",
      "I: 30 Train error: 0.996516667364948% Train accuracy: 0.108 Test-Err:: 0.09965166673649481% Test-Acc:: 0.0108\n"
     ]
    },
    {
     "ename": "InterruptException",
     "evalue": "InterruptException:",
     "output_type": "error",
     "traceback": [
      "InterruptException:",
      "",
      "Stacktrace:",
      " [1] broadcasted(::Function, ::Array{Real,2}) at ./broadcast.jl:1232",
      " [2] top-level scope at ./In[130]:26"
     ]
    }
   ],
   "source": [
    "using Random\n",
    "Random.seed!(1)\n",
    "\n",
    "relu(x) = x > 0 ? x : 0\n",
    "relu2deriv(x) = x > 0 ? 1 : 0\n",
    "\n",
    "alpha, iterations, hidden_size = (0.005, 300, 100)\n",
    "pixels_per_image, num_labels = (784, 10)\n",
    "\n",
    "weights_0_1 = 0.2 .* rand(pixels_per_image,hidden_size) .- 0.1\n",
    "weights_1_2 = 0.2 .* rand(hidden_size,num_labels) .- 0.1\n",
    "\n",
    "for j = 1:iterations\n",
    "    Error, Correct_cnt = (0.0, 0)\n",
    "    for i = 1:size(images)[1]\n",
    "        layer_0 = images[i,:]\n",
    "        layer_1 = relu.(layer_0' * weights_0_1)\n",
    "        dropout_mask = bitrand(size(layer_1))\n",
    "        layer_1 .*= dropout_mask .* 2\n",
    "        layer_2 = layer_1 * weights_1_2\n",
    "\n",
    "        Error += sum((labels[i,:]' .- layer_2) .^ 2)\n",
    "        Correct_cnt += Int(argmax(layer_2) == argmax(labels[i,:]'))\n",
    "\n",
    "        layer_2_delta = (labels[i]' .- layer_2)\n",
    "        layer_1_delta = (layer_2_delta * weights_1_2') .* relu2deriv.(layer_1)\n",
    "\n",
    "        layer_1_delta .*= dropout_mask\n",
    "        \n",
    "        weights_1_2 += alpha .* layer_1' * layer_2_delta\n",
    "        weights_0_1 += alpha .* layer_0 * layer_1_delta\n",
    "    end\n",
    "    \n",
    "    if (j % 10 == 0)\n",
    "        test_Error, test_Correct_cnt = (0.0, 0)\n",
    "        for i = 1:size(test_images)[1]\n",
    "        \n",
    "            layer_0 = test_images[i,:]\n",
    "            layer_1 = relu.(layer_0' * weights_0_1)\n",
    "            layer_2 = layer_1 * weights_1_2\n",
    "\n",
    "            test_Error += sum((test_labels[i,:]' .- layer_2) .^ 2)\n",
    "            test_Correct_cnt += Int(argmax(layer_2) == argmax(test_labels[i,:]'))\n",
    "        end\n",
    "        println(\"I: $(j) Train error: $(Error/size(images)[1])% Train accuracy: $(Correct_cnt/size(images)[1]) Test-Err:: $(Error/size(test_images)[1])% Test-Acc:: $(Correct_cnt/size(test_images)[1])\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "Random.seed!(1)\n",
    "\n",
    "relu(x) = x > 0 ? x : 0\n",
    "relu2deriv(x) = x > 0 ? 1 : 0\n",
    "\n",
    "batch_size = 100\n",
    "alpha, iterations = (0.001, 300)\n",
    "pixels_per_image, num_labels, hidden_size = (784, 10, 100)\n",
    "\n",
    "weights_0_1 = 0.2 .* rand(pixels_per_image,hidden_size) .- 0.1\n",
    "weights_1_2 = 0.2 .* rand(hidden_size,num_labels) .- 0.1\n",
    "\n",
    "for j = 1:iterations\n",
    "    Error, Correct_cnt = (0.0, 0)\n",
    "    for i = 1:batch_size:size(images)[1]-batch_size\n",
    "        batch_start, batch_end = i, i+batch_size-1\n",
    "        layer_0 = images[batch_start:batch_end,:]\n",
    "        layer_1 = relu.(layer_0 * weights_0_1)\n",
    "        dropout_mask = bitrand(size(layer_1))\n",
    "        layer_1 .*= (dropout_mask .* 2)\n",
    "        layer_2 = layer_1 * weights_1_2\n",
    "        \n",
    "        Error += sum((labels[batch_start:batch_end,:] .- layer_2) .^ 2)\n",
    "        \n",
    "        for k=1:batch_size\n",
    "            Correct_cnt += Int(argmax(layer_2[k,:]) == argmax(labels[batch_start+k-1,:]))\n",
    "            layer_2_delta = (labels[batch_start:batch_end,:] .- layer_2) ./batch_size\n",
    "            layer_1_delta = (layer_2_delta * weights_1_2') .* relu2deriv.(layer_1)\n",
    "\n",
    "            layer_1_delta .*= dropout_mask\n",
    "\n",
    "            weights_1_2 += alpha .* layer_1 * layer_2_delta\n",
    "            \n",
    "            println(size(layer_1), size(layer_2_delta))\n",
    "            @assert false\n",
    "            weights_0_1 += alpha .* layer_0' * layer_1_delta\n",
    "        end\n",
    "    end\n",
    "        \n",
    "    if (j % 10 == 0)\n",
    "        test_Error, test_Correct_cnt = (0.0, 0)\n",
    "        for i = 1:size(test_images)[1]\n",
    "\n",
    "            layer_0 = test_images[i,:]\n",
    "            layer_1 = relu.(layer_0' * weights_0_1)\n",
    "            layer_2 = layer_1 * weights_1_2\n",
    "\n",
    "            test_Error += sum((test_labels[i,:]' .- layer_2) .^ 2)\n",
    "            test_Correct_cnt += Int(argmax(layer_2) == argmax(test_labels[i,:]'))\n",
    "        end\n",
    "        println(\"I: $(j) Train error: $(Error/size(images)[1])% Train accuracy: $(Correct_cnt/size(images)[1]) Test-Err:: $(test_Error/size(test_images)[1])% Test-Acc:: $(test_Correct_cnt/size(test_images)[1])\")\n",
    "    end\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.0",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
