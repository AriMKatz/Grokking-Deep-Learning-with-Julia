{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Introduction to Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Base:println,+\n",
    "\n",
    "mutable struct Tensor\n",
    "    data \n",
    "end\n",
    "\n",
    "+(a::Tensor, b::Tensor) = a.data + b.data\n",
    "\n",
    "println(t::Tensor) = println(t.data)\n",
    "    \n",
    "x = Tensor([1,2,3,4,5])\n",
    "print(x)\n",
    "\n",
    "y = x + x\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function workspace()\n",
    "   atexit() do\n",
    "       run(`$(Base.julia_cmd())`)\n",
    "   end\n",
    "   exit()\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Introduction to Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Base:println,+\n",
    "\n",
    "mutable struct Tensor\n",
    "    data\n",
    "    creators\n",
    "    creation_op\n",
    "    grad \n",
    "    Tensor(data; creators=nothing, creation_op = nothing) = \n",
    "    new(data, creators, creation_op)\n",
    "end\n",
    "\n",
    "function backward(t::Tensor, grad)\n",
    "    t.grad = grad\n",
    "    \n",
    "    if t.creation_op == \"add\"\n",
    "        backward(t.creators[1], grad)\n",
    "        backward(t.creators[2], grad)\n",
    "    end\n",
    "end\n",
    "\n",
    "+(a::Tensor, b::Tensor) = Tensor(a.data + b.data; creators=[a,b], creation_op=\"add\")\n",
    "println(t::Tensor) = println(t.data)\n",
    "println(t::Array{Tensor,1}) = println([i.data for i in t])\n",
    "    \n",
    "x = Tensor([1,2,3,4,5])\n",
    "y = Tensor([2,2,2,2,2])\n",
    "\n",
    "z = x + y\n",
    "backward(z, Tensor([1,1,1,1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(x.grad)\n",
    "println(y.grad)\n",
    "println(z.creators)\n",
    "println(z.creation_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Tensor([1,2,3,4,5])\n",
    "b = Tensor([2,2,2,2,2])\n",
    "c = Tensor([5,4,3,2,1])\n",
    "d = Tensor([-1,-2,-3,-4,-5])\n",
    "\n",
    "e = a + b\n",
    "f = c + d\n",
    "g = e + f\n",
    "\n",
    "backward(g, Tensor([1,1,1,1,1]))\n",
    "\n",
    "println(a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Tensors That Are Used Multiple Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Tensor([1,2,3,4,5])\n",
    "b = Tensor([2,2,2,2,2])\n",
    "c = Tensor([5,4,3,2,1])\n",
    "\n",
    "d = a + b\n",
    "e = b + c\n",
    "f = d + e\n",
    "backward(f, Tensor([1,1,1,1,1]))\n",
    "\n",
    "b.grad.data == [2,2,2,2,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.grad.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Upgrading Autograd to Support Multiple Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "import Base:+,println\n",
    "\n",
    "mutable struct Tensor\n",
    "    data\n",
    "    autograd\n",
    "    creators\n",
    "    creation_op\n",
    "    id\n",
    "    children\n",
    "    grad \n",
    "    function Tensor(data; autograd=false, creators=nothing, creation_op = nothing, id=nothing)\n",
    "        if isnothing(id)\n",
    "            id = rand(1:100000)\n",
    "        end\n",
    "        T = new(data, autograd, creators, creation_op, id)\n",
    "        T.children = Dict()\n",
    "        T.grad = nothing\n",
    "        \n",
    "        if !(isnothing(creators))\n",
    "            for c in creators\n",
    "                if haskey(c.children, T.id)\n",
    "                    c.children[T.id] += 1\n",
    "                else\n",
    "                    c.children[T.id] = 1\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        return T\n",
    "    end\n",
    "end\n",
    "\n",
    "function all_children_grads_accounted_for(t::Tensor)\n",
    "    for (id, cnt) in t.children\n",
    "        if (cnt != 0)\n",
    "            return false\n",
    "        end\n",
    "    end\n",
    "    return true\n",
    "end\n",
    "\n",
    "function backward(t::Tensor, grad=nothing, grad_origin=nothing)\n",
    "    if t.autograd\n",
    "        grad = Tensor(ones(size(t.data)))\n",
    "    \n",
    "        if !(isnothing(grad_origin))\n",
    "            if t.children[grad_origin.id] == 0\n",
    "                throw(\"cannot backprop more than once\")\n",
    "            else\n",
    "                t.children[grad_origin.id] -= 1\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        if isnothing(t.grad)\n",
    "            t.grad = grad\n",
    "        else\n",
    "            t.grad += grad\n",
    "        end\n",
    "        \n",
    "        # grads must not have grads of their own\n",
    "        @assert !grad.autograd\n",
    "        \n",
    "        # only continue backpropping if there's something to\n",
    "        # backprop into and if all gradients (from children)\n",
    "        # are accounted for override waiting for children if\n",
    "        # \"backprop\" was called on this variable directly\n",
    "        \n",
    "        if (!isnothing(t.creators) && (all_children_grads_accounted_for(t) || isnothing(grad_origin)))\n",
    "            if t.creation_op == \"add\"\n",
    "                backward(t.creators[1], t.grad, t)\n",
    "                backward(t.creators[2], t.grad, t)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "function +(a::Tensor, b::Tensor)\n",
    "    if (a.autograd && b.autograd)\n",
    "        return Tensor(a.data .+ b.data; autograd=true, creators=[a,b], creation_op = \"add\")\n",
    "    end\n",
    "    return Tensor(a.data+b.data)\n",
    "end\n",
    "\n",
    "println(t::Tensor) = println(t.data)\n",
    "\n",
    "a = Tensor([1,2,3,4,5]; autograd=true)\n",
    "b = Tensor([2,2,2,2,2]; autograd=true)\n",
    "c = Tensor([5,4,3,2,1]; autograd=true)\n",
    "\n",
    "d = a + b\n",
    "e = b + c\n",
    "f = d + e\n",
    "\n",
    "backward(f, Tensor([1,1,1,1,1]))\n",
    "\n",
    "println(b.grad.data == [2,2,2,2,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Add Support for Negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "import Base:+,-,println\n",
    "\n",
    "mutable struct Tensor\n",
    "    data\n",
    "    autograd\n",
    "    creators\n",
    "    creation_op\n",
    "    id\n",
    "    children\n",
    "    grad \n",
    "    function Tensor(data; autograd=false, creators=nothing, creation_op = nothing, id=nothing)\n",
    "        if isnothing(id)\n",
    "            id = rand(1:100000)\n",
    "        end\n",
    "        T = new(data, autograd, creators, creation_op, id)\n",
    "        T.children = Dict()\n",
    "        T.grad = nothing\n",
    "        \n",
    "        if !(isnothing(creators))\n",
    "            for c in creators\n",
    "                if haskey(c.children, T.id)\n",
    "                    c.children[T.id] += 1\n",
    "                else\n",
    "                    c.children[T.id] = 1\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        return T\n",
    "    end\n",
    "end\n",
    "\n",
    "function all_children_grads_accounted_for(t::Tensor)\n",
    "    for (id, cnt) in t.children\n",
    "        if (cnt != 0)\n",
    "            return false\n",
    "        end\n",
    "    end\n",
    "    return true\n",
    "end\n",
    "\n",
    "function backward(t::Tensor, grad=nothing, grad_origin=nothing)\n",
    "    if t.autograd\n",
    "        if isnothing(grad)\n",
    "            grad = Tensor(ones(size(t.data)))\n",
    "        end\n",
    "    \n",
    "        if !(isnothing(grad_origin))\n",
    "            if t.children[grad_origin.id] == 0\n",
    "                throw(\"cannot backprop more than once\")\n",
    "            else\n",
    "                t.children[grad_origin.id] -= 1\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        if isnothing(t.grad)\n",
    "            t.grad = grad\n",
    "        else\n",
    "            t.grad += grad\n",
    "        end\n",
    "        \n",
    "        # grads must not have grads of their own\n",
    "        @assert !grad.autograd\n",
    "        \n",
    "        # only continue backpropping if there's something to\n",
    "        # backprop into and if all gradients (from children)\n",
    "        # are accounted for override waiting for children if\n",
    "        # \"backprop\" was called on this variable directly\n",
    "        \n",
    "        if (!isnothing(t.creators) && (all_children_grads_accounted_for(t) || isnothing(grad_origin)))\n",
    "            if t.creation_op == \"add\"\n",
    "                backward(t.creators[1], t.grad, t)\n",
    "                backward(t.creators[2], t.grad, t)\n",
    "            end\n",
    "            if t.creation_op == \"neg\"\n",
    "                backward(t.creators[1], -t.grad)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "function +(a::Tensor, b::Tensor)\n",
    "    if (a.autograd && b.autograd)\n",
    "        return Tensor(a.data .+ b.data; autograd=true, creators=[a,b], creation_op = \"add\")\n",
    "    end\n",
    "    return Tensor(a.data+b.data)\n",
    "end\n",
    "\n",
    "function -(a::Tensor)\n",
    "    if (a.autograd)\n",
    "        return Tensor(a.data .* -1; autograd=true, creators=[a], creation_op = \"neg\")\n",
    "    end\n",
    "    return Tensor(a.data .* -1)\n",
    "end\n",
    "\n",
    "\n",
    "println(t::Tensor) = println(t.data)\n",
    "\n",
    "a = Tensor([1,2,3,4,5]; autograd=true)\n",
    "b = Tensor([2,2,2,2,2]; autograd=true)\n",
    "c = Tensor([5,4,3,2,1]; autograd=true)\n",
    "\n",
    "d = a + (-b)\n",
    "e = (-b) + c\n",
    "f = d + e\n",
    "\n",
    "backward(f, Tensor([1,1,1,1,1]))\n",
    "\n",
    "print(b.grad.data == [-2,-2,-2,-2,-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: Add Support for Additional Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add on 33852\n",
      "add on 1859\n",
      "neg\n",
      "add on 92377\n",
      "neg\n",
      "true"
     ]
    }
   ],
   "source": [
    "using Random\n",
    "import Base:+,-,*,println, sum, broadcasted, size, adjoint, display\n",
    "\n",
    "mutable struct Tensor\n",
    "    data\n",
    "    autograd\n",
    "    creators\n",
    "    creation_op\n",
    "    id\n",
    "    children\n",
    "    grad \n",
    "    function Tensor(data; autograd=false, creators=nothing, creation_op = nothing, id=nothing)\n",
    "        if isnothing(id)\n",
    "            id = rand(1:100000)\n",
    "        end\n",
    "        T = new(data, autograd, creators, creation_op, id)\n",
    "        T.children = Dict()\n",
    "        T.grad = nothing\n",
    "        \n",
    "        if !(isnothing(creators))\n",
    "            for c in creators\n",
    "                if haskey(c.children, T.id)\n",
    "                    c.children[T.id] += 1\n",
    "                else\n",
    "                    c.children[T.id] = 1\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        return T\n",
    "    end\n",
    "end\n",
    "\n",
    "function all_children_grads_accounted_for(t::Tensor)\n",
    "    for (id, cnt) in t.children\n",
    "        if (cnt != 0)\n",
    "            return false\n",
    "        end\n",
    "    end\n",
    "    return true\n",
    "end\n",
    "\n",
    "function backward(t::Tensor, grad=nothing, grad_origin=nothing)\n",
    "    if t.autograd\n",
    "        if isnothing(grad)\n",
    "            grad = Tensor(ones(size(t.data)))\n",
    "        end\n",
    "    \n",
    "        if !(isnothing(grad_origin))\n",
    "            if t.children[grad_origin.id] == 0\n",
    "                throw(\"cannot backprop more than once\")\n",
    "            else\n",
    "                t.children[grad_origin.id] -= 1\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        if isnothing(t.grad)\n",
    "            t.grad = grad\n",
    "        else\n",
    "            t.grad += grad\n",
    "        end\n",
    "        \n",
    "        # grads must not have grads of their own\n",
    "        @assert !grad.autograd\n",
    "        \n",
    "        # only continue backpropping if there's something to\n",
    "        # backprop into and if all gradients (from children)\n",
    "        # are accounted for override waiting for children if\n",
    "        # \"backprop\" was called on this variable directly\n",
    "        \n",
    "        if (!isnothing(t.creators) && (all_children_grads_accounted_for(t) || isnothing(grad_origin)))\n",
    "            if t.creation_op == \"add\"\n",
    "                println(\"add on $(t.id)\")\n",
    "                backward(t.creators[1], t.grad, t)\n",
    "                backward(t.creators[2], t.grad, t)\n",
    "            end\n",
    "            \n",
    "            if t.creation_op == \"sub\"\n",
    "                println(\"sub on $(t.id)\")\n",
    "                backward(t.creators[1], t.grad, t)\n",
    "                backward(t.creators[2], -t.grad, t)\n",
    "            end\n",
    "            \n",
    "            if t.creation_op == \"mul\"\n",
    "                println(\"mul on $(t.id)\")\n",
    "                new_ = t.grad .* t.creators[2]\n",
    "                backward(t.creators[1], new_, t)\n",
    "                new_ = t.grad .* t.creators[1]\n",
    "                backward(t.creators[2], new_, t)\n",
    "            end\n",
    "            \n",
    "            if t.creation_op == \"mm\"\n",
    "                println(\"mm on $(t.id)\")\n",
    "                c1 = t.creators[1]\n",
    "                c2 = t.creators[2]\n",
    "                new_ =  t.grad * c2' ################\n",
    "                backward(c1, new_)\n",
    "                new_ = c1' * t.grad\n",
    "                backward(c2, new_)\n",
    "            end\n",
    "                  \n",
    "            if t.creation_op == \"transpose\"\n",
    "                println(\"transpose on $(t.id)\")\n",
    "                backward(t.creators[1], t.grad')\n",
    "            end\n",
    "            \n",
    "            if occursin(\"sum\", t.creation_op)\n",
    "                println(\"sum on $(t.id)\")\n",
    "                dim = parse(Int, split(t.creation_op, \"_\")[2])\n",
    "                backward(t.creators[1], expand(t.grad, dim, size(t.creators[1].data)[dim]))\n",
    "            end\n",
    "            \n",
    "            if occursin(\"expand\", t.creation_op)\n",
    "                println(\"expand on $(t.id)\")\n",
    "                dim = parse(Int, split(t.creation_op, \"_\")[2])\n",
    "                backward(t.creators[1], sum(t.grad;dims=dim))\n",
    "            end              \n",
    "            \n",
    "            if t.creation_op == \"neg\"\n",
    "                println(\"neg\")\n",
    "                backward(t.creators[1], -t.grad)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "size(a::Tensor) = size(a.data)\n",
    "\n",
    "function +(a::Tensor, b::Tensor)\n",
    "    if (a.autograd && b.autograd)\n",
    "        return Tensor(a.data + b.data; autograd=true, creators=[a,b], creation_op = \"add\")\n",
    "    end\n",
    "    return Tensor(a.data+b.data)\n",
    "end\n",
    "\n",
    "function -(a::Tensor)\n",
    "    if (a.autograd)\n",
    "        return Tensor(a.data .* -1; autograd=true, creators=[a], creation_op = \"neg\")\n",
    "    end\n",
    "    return Tensor(a.data .* -1)\n",
    "end\n",
    "\n",
    "function -(a::Tensor, b::Tensor)\n",
    "    if (a.autograd && b.autograd)\n",
    "        return Tensor(a.data - b.data; autograd=true, creators=[a,b], creation_op = \"sub\")\n",
    "    end\n",
    "    return Tensor(a.data-b.data)\n",
    "end\n",
    "\n",
    "#element-wise multiplication\n",
    "function broadcasted(*, a::Tensor, b::Tensor)\n",
    "    new_data = a.data\n",
    "    for i=1:length(new_data)\n",
    "#         println(b.data[i],\"  \", new_data[i])\n",
    "        new_data[i] *= b.data[i]\n",
    "    end\n",
    "    if (a.autograd && b.autograd)\n",
    "        return Tensor(new_data; autograd=true, creators=[a,b], creation_op =\"mul\")\n",
    "    end\n",
    "    return Tensor(new_data)\n",
    "end\n",
    "\n",
    "function sum(a::Tensor; dims=1)\n",
    "    new_ = dropdims(sum(a.data ;dims=dims), dims = tuple(findall(size(a) .== 1)...))\n",
    "    if (a.autograd && b.autograd)\n",
    "        return Tensor(new_; autograd=true, creators=[a], creation_op = \"sum_\"*string(dims))\n",
    "    end\n",
    "    return Tensor(sum(new_;dims=dims))\n",
    "end\n",
    "\n",
    "function expand(a::Tensor, dim, copies)\n",
    "    sz = size(a)\n",
    "    rep = ntuple(d->d==dim ? copies : 1, length(sz)+1)\n",
    "    new_size = ntuple(d->d<dim ? sz[d] : d == dim ? 1 : sz[d-1], length(sz)+1)\n",
    "    new_data =  repeat(reshape(a.data, new_size), outer=rep)\n",
    "    if (a.autograd)\n",
    "        return Tensor(new_data; autograd=true, creators=[a], creation_op = \"expand_\"*string(dim))\n",
    "    end\n",
    "    return Tensor(new_data)\n",
    "end\n",
    "\n",
    "#transpose\n",
    "function adjoint(a::Tensor)\n",
    "    if (a.autograd)\n",
    "        return Tensor(a.data';autograd=true, creators=[a], creation_op = \"transpose\")\n",
    "    end\n",
    "    return Tensor(a.data')\n",
    "end\n",
    "\n",
    "#matrix multiply \n",
    "function *(a::Tensor, b::Tensor)\n",
    "    if (a.autograd && b.autograd)\n",
    "        return Tensor(a.data * b.data; autograd=true, creators=[a,b], creation_op = \"mm\")\n",
    "    end\n",
    "    return Tensor(a.data * b.data)\n",
    "end\n",
    "\n",
    "    \n",
    "println(t::Tensor) = println(t.data)\n",
    "display(t::Tensor) = display(t.data)\n",
    "\n",
    "a = Tensor([1,2,3,4,5]; autograd=true)\n",
    "b = Tensor([2,2,2,2,2]; autograd=true)\n",
    "c = Tensor([5,4,3,2,1]; autograd=true)\n",
    "\n",
    "d = a + (-b)\n",
    "e = (-b) + c\n",
    "f = d + e\n",
    "\n",
    "backward(f, Tensor([1,1,1,1,1]))\n",
    "\n",
    "print(b.grad.data == [-2,-2,-2,-2,-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Tensor([1 2 3;4 5 6];autograd=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sum(x;dims=2).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_expand = expand(x,3,4).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_expand[1,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_expand[2,:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7: Use Autograd to Train a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Previously we would train a model like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.319677517363771\n",
      "0.645370221071993\n",
      "0.48046558059136113\n",
      "0.42673209144608215\n",
      "0.3930618130835104\n",
      "0.363594896295545\n",
      "0.33605797739677096\n",
      "0.3100545140899324\n",
      "0.28544274299374384\n",
      "0.2621351937537131\n"
     ]
    }
   ],
   "source": [
    "using Random: seed!\n",
    "seed!(0)\n",
    "\n",
    "data = [ 0 0; 0 1; 1 0; 1 1;]\n",
    "target = [0; 1; 0; 1]\n",
    "\n",
    "weights_0_1 = rand(2,3)\n",
    "weights_1_2 = rand(3,1)\n",
    "\n",
    "for i=1:10\n",
    "    \n",
    "#     # Predict\n",
    "    layer_1 = data * weights_0_1\n",
    "    layer_2 = layer_1 * weights_1_2\n",
    "    \n",
    "#     # Compare\n",
    "    diff = (layer_2 - target)\n",
    "    sqdiff = (diff .* diff)\n",
    "    loss = sum(sqdiff;dims=1) # mean squared error loss\n",
    "\n",
    "#     # Learn: this is the backpropagation piece\n",
    "    layer_1_grad = diff * weights_1_2'\n",
    "    weight_1_2_update = layer_1' * diff\n",
    "    weight_0_1_update = data' * layer_1_grad\n",
    "    \n",
    "    weights_1_2 .-= weight_1_2_update .* 0.1\n",
    "    weights_0_1 .-= weight_0_1_update .* 0.1\n",
    "    println(loss[1])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random: seed!\n",
    "seed!(0)\n",
    "\n",
    "data = Tensor([ 0 0; 0 1; 1 0; 1 1;], autograd=true)\n",
    "target = Tensor([0; 1; 0; 1], autograd=true)\n",
    "\n",
    "w = []\n",
    "push!(w, Tensor(rand(2,3), autograd=true))\n",
    "push!(w, Tensor(rand(3,1), autograd=true))\n",
    "\n",
    "for i=1:10\n",
    "\n",
    "#     # Predict\n",
    "    pred_1 = data * w[1]\n",
    "    pred_2 = pred_1 * w[2]\n",
    "    diff_1 = pred_2 .- target\n",
    "    diff_2 = diff_1 .* diff_1\n",
    "    \n",
    "#     # Compare\n",
    "    loss = sum(diff_2;dims=1)\n",
    "    \n",
    "#     # Learn\n",
    "    backward(loss, Tensor(ones(Float32, size(loss.data))))\n",
    "\n",
    "    for w_ in w\n",
    "        w_.data .-= w_.grad.data .* 0.1\n",
    "        w_.grad.data .*= 0\n",
    "    end\n",
    "\n",
    "    println(loss)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.0",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
